{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"models/huggingface\"\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"models/huggingface/hub\"\n",
    "os.environ[\"HF_ASSETS_CACHE\"] = \"models/huggingface/assets\"\n",
    "os.environ[\"HF_TOKEN_PATH\"] = \"models/huggingface/token\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "model_names = [\n",
    "    \"tabularisai/multilingual-sentiment-analysis\", #Fast\n",
    "    # \"nlptown/bert-base-multilingual-uncased-sentiment\" #Slow\n",
    "    ]\n",
    "tokenizers = [AutoTokenizer.from_pretrained(model_name) for model_name in model_names]\n",
    "models = [(AutoModelForSequenceClassification.from_pretrained(model_name)).to(device) for model_name in model_names]\n",
    "sentiment_map = {0: \"Very Negative\", 1: \"Negative\", 2: \"Neutral\", 3: \"Positive\", 4: \"Very Positive\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(models: list[PreTrainedModel], tokenizers: list[PreTrainedTokenizerFast], texts: list[str]):\n",
    " \n",
    "    sentiment_map = {0: \"Very Negative\", 1: \"Negative\", 2: \"Neutral\", 3: \"Positive\", 4: \"Very Positive\"}\n",
    "\n",
    "    # Tensor to store probabilities (num_texts x num_models x num_sentiments)\n",
    "    tensor = torch.zeros(len(texts), len(models), len(sentiment_map)).to(device)\n",
    "    \n",
    "    for model_idx, (model, tokenizer) in enumerate(zip(models, tokenizers)):\n",
    "        # Tokenize the input texts\n",
    "        \n",
    "        inputs = tokenizer.batch_encode_plus(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)#, max_length=512)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Get model outputs\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Compute probabilities\n",
    "        probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)  # Shape: (num_texts x num_sentiments)\n",
    "\n",
    "        # Assign probabilities to the tensor for the current model\n",
    "        # print(probabilities)\n",
    "        tensor[:, model_idx, :] = probabilities\n",
    "\n",
    "    # Aggregate probabilities across models (mean)\n",
    "    agg_probabilities = tensor.mean(dim=1)  # Shape: (num_texts x num_sentiments)\n",
    "    # print(agg_probabilities)\n",
    "    # Get final sentiment predictions\n",
    "    predicted_indices = torch.argmax(agg_probabilities, dim=-1)  # Shape: (num_texts)\n",
    "\n",
    "    # Map indices to sentiment labels\n",
    "    classification = [sentiment_map[p.item()] for p in predicted_indices]\n",
    "\n",
    "    return classification\n",
    "\n",
    "def run_batch(models: list[PreTrainedModel], tokenizers: list[PreTrainedTokenizerFast], texts: list[str], batch_size: int = 64):\n",
    "    \n",
    "    classifications = [None] * len(texts)  # List to store predictions\n",
    "\n",
    "    for batch_idx in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[batch_idx: batch_idx + batch_size]  # Get the current batch of texts\n",
    "\n",
    "        # Predict sentiment for the batch\n",
    "        batch_classifications = predict_sentiment(models, tokenizers, batch_texts)\n",
    "        \n",
    "        # Assign the batch's classifications to the corresponding indices\n",
    "        classifications[batch_idx: batch_idx + batch_size] = batch_classifications\n",
    "\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()  # Clear CUDA cache after processing each batch (optional)\n",
    "\n",
    "    return classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment_with_chunking(models: list[PreTrainedModel], \n",
    "                                    tokenizers: list[PreTrainedTokenizerFast], \n",
    "                                    texts: list[str], \n",
    "                                    chunk_size: int = 512):\n",
    "    \"\"\"\n",
    "    Predict sentiment for a list of texts, chunking long texts into manageable sizes.\n",
    "    \"\"\"\n",
    "    sentiment_map = {0: \"Very Negative\", 1: \"Negative\", 2: \"Neutral\", 3: \"Positive\", 4: \"Very Positive\"}\n",
    "\n",
    "    # Tensor to store probabilities (num_texts x num_models x num_sentiments)\n",
    "    tensor = torch.zeros(len(texts), len(models), len(sentiment_map)).to(device)\n",
    "    \n",
    "    for model_idx, (model, tokenizer) in enumerate(zip(models, tokenizers)):\n",
    "        for text_idx, text in enumerate(texts):\n",
    "            # Tokenize and chunk the text if necessary\n",
    "            inputs = tokenizer.encode_plus(text, truncation=False, return_tensors=\"pt\").to(device)\n",
    "            input_ids = inputs['input_ids'].squeeze(0)\n",
    "            num_chunks = (len(input_ids) + chunk_size - 1) // chunk_size  # Calculate required chunks\n",
    "            \n",
    "            chunk_probabilities = []\n",
    "            \n",
    "            for i in range(num_chunks):\n",
    "                chunk_input_ids = input_ids[i * chunk_size: (i + 1) * chunk_size]\n",
    "                chunk_attention_mask = torch.ones_like(chunk_input_ids).to(device)\n",
    "                \n",
    "                chunk_inputs = {\n",
    "                    'input_ids': chunk_input_ids.unsqueeze(0),\n",
    "                    'attention_mask': chunk_attention_mask.unsqueeze(0)\n",
    "                }\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**chunk_inputs)\n",
    "                \n",
    "                probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)  # Shape: (1 x num_sentiments)\n",
    "                chunk_probabilities.append(probabilities)\n",
    "            \n",
    "            # Aggregate probabilities across chunks\n",
    "            aggregated_probabilities = torch.mean(torch.stack(chunk_probabilities, dim=0), dim=0)\n",
    "            \n",
    "            # Assign probabilities for the current text and model\n",
    "            tensor[text_idx, model_idx, :] = aggregated_probabilities\n",
    "\n",
    "    # Aggregate probabilities across models (mean)\n",
    "    agg_probabilities = tensor.mean(dim=1)  # Shape: (num_texts x num_sentiments)\n",
    "\n",
    "    # Get final sentiment predictions\n",
    "    predicted_indices = torch.argmax(agg_probabilities, dim=-1)  # Shape: (num_texts)\n",
    "\n",
    "    # Map indices to sentiment labels\n",
    "    classification = [sentiment_map[p.item()] for p in predicted_indices]\n",
    "\n",
    "    return classification\n",
    "\n",
    "\n",
    "def run_batch_with_chunking(models: list[PreTrainedModel], \n",
    "                            tokenizers: list[PreTrainedTokenizerFast], \n",
    "                            texts: list[str], \n",
    "                            batch_size: int = 64, \n",
    "                            chunk_size: int = 512):\n",
    "    \"\"\"\n",
    "    Run predictions in batches with support for chunking long texts.\n",
    "    \"\"\"\n",
    "    classifications = [None] * len(texts)  # List to store predictions\n",
    "\n",
    "    for batch_idx in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[batch_idx: batch_idx + batch_size]  # Get the current batch of texts\n",
    "\n",
    "        # Predict sentiment for the batch\n",
    "        batch_classifications = predict_sentiment_with_chunking(models, tokenizers, batch_texts, chunk_size)\n",
    "        \n",
    "        # Assign the batch's classifications to the corresponding indices\n",
    "        classifications[batch_idx: batch_idx + batch_size] = batch_classifications\n",
    "\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()  # Clear CUDA cache after processing each batch (optional)\n",
    "\n",
    "    return classifications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment_with_chunking(models: list[PreTrainedModel], \n",
    "                                    tokenizers: list[PreTrainedTokenizerFast], \n",
    "                                    texts: list[str], \n",
    "                                    chunk_size: int = 512):\n",
    "    \"\"\"\n",
    "    Predict sentiment for a list of texts, chunking long texts into manageable sizes.\n",
    "    \"\"\"\n",
    "    sentiment_map = {0: \"Very Negative\", 1: \"Negative\", 2: \"Neutral\", 3: \"Positive\", 4: \"Very Positive\"}\n",
    "\n",
    "    # Tensor to store probabilities (num_texts x num_models x num_sentiments)\n",
    "    tensor = torch.zeros(len(texts), len(models), len(sentiment_map)).to(device)\n",
    "    \n",
    "    for model_idx, (model, tokenizer) in enumerate(zip(models, tokenizers)):\n",
    "        for text_idx, text in enumerate(texts):\n",
    "            # Tokenize and chunk the text if necessary\n",
    "            inputs = tokenizer.encode_plus(text, truncation=False, return_tensors=\"pt\").to(device)\n",
    "            input_ids = inputs['input_ids'].squeeze(0)\n",
    "            num_chunks = (len(input_ids) + chunk_size - 1) // chunk_size  # Calculate required chunks\n",
    "            \n",
    "            chunk_probabilities = []\n",
    "            \n",
    "            for i in range(num_chunks):\n",
    "                chunk_input_ids = input_ids[i * chunk_size: (i + 1) * chunk_size]\n",
    "                chunk_attention_mask = torch.ones_like(chunk_input_ids).to(device)\n",
    "                \n",
    "                chunk_inputs = {\n",
    "                    'input_ids': chunk_input_ids.unsqueeze(0),\n",
    "                    'attention_mask': chunk_attention_mask.unsqueeze(0)\n",
    "                }\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**chunk_inputs)\n",
    "                \n",
    "                probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)  # Shape: (1 x num_sentiments)\n",
    "                chunk_probabilities.append(probabilities)\n",
    "            \n",
    "            # Aggregate probabilities across chunks\n",
    "            aggregated_probabilities = torch.mean(torch.stack(chunk_probabilities, dim=0), dim=0)\n",
    "            \n",
    "            # Assign probabilities for the current text and model\n",
    "            tensor[text_idx, model_idx, :] = aggregated_probabilities\n",
    "\n",
    "    # Aggregate probabilities across models (mean)\n",
    "    agg_probabilities = tensor.mean(dim=1)  # Shape: (num_texts x num_sentiments)\n",
    "\n",
    "    # Get final sentiment predictions\n",
    "    predicted_indices = torch.argmax(agg_probabilities, dim=-1)  # Shape: (num_texts)\n",
    "\n",
    "    # Map indices to sentiment labels\n",
    "    classification = [sentiment_map[p.item()] for p in predicted_indices]\n",
    "\n",
    "    return classification\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
