{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"models/huggingface\"\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"models/huggingface/hub\"\n",
    "os.environ[\"HF_ASSETS_CACHE\"] = \"models/huggingface/assets\"\n",
    "os.environ[\"HF_TOKEN_PATH\"] = \"models/huggingface/token\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "model_name = \"tabularisai/multilingual-sentiment-analysis\"\n",
    "\n",
    "tokenizers = AutoTokenizer.from_pretrained(model_name)\n",
    "models = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "sentiment_map = {0: \"Very Negative\", 1: \"Negative\", 2: \"Neutral\", 3: \"Positive\", 4: \"Very Positive\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros(3, 12).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, x, y, z = predict_sentiment_single_record(models, tokenizers, \"happy yipee HURRAH happy yayyyy thank you yayyyyy happy happy happy happy yayyyy thank you yayyyyy BAD BAD\")\n",
    "w, x, y , z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment_single_record(\n",
    "        model: PreTrainedModel, \n",
    "        tokenizer: PreTrainedTokenizerFast,\n",
    "        text: str, \n",
    "        chunk_size: int = 10,\n",
    "        device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "):\n",
    "    sentiment_map = {0: \"Very Negative\", 1: \"Negative\", 2: \"Neutral\", 3: \"Positive\", 4: \"Very Positive\"}\n",
    "\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer.encode_plus(text, truncation=False, return_tensors=\"pt\", padding=False).to(device)\n",
    "    input_ids = inputs['input_ids'].squeeze(0)\n",
    "    attention_mask = inputs['attention_mask'].squeeze(0)\n",
    "\n",
    "    # If the text is shorter than the chunk size, adjust the chunk size\n",
    "    if len(input_ids) < chunk_size:\n",
    "        chunk_size = len(input_ids)\n",
    "    \n",
    "    # Calculate the number of chunks needed\n",
    "    num_chunks = (len(input_ids) + chunk_size - 1) // chunk_size  # Calculate required chunks\n",
    "\n",
    "    # Prepare tensors for chunked input\n",
    "    chunk_inputs = {\n",
    "        'input_ids': torch.zeros(num_chunks, chunk_size, dtype=torch.long).to(device),  # Ensure dtype is torch.long\n",
    "        'attention_mask': torch.zeros(num_chunks, chunk_size, dtype=torch.long).to(device)  # Ensure dtype is torch.long\n",
    "    }\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        # Extract the corresponding chunk of tokens and attention masks\n",
    "        chunk_input_ids = input_ids[i * chunk_size: (i + 1) * chunk_size]\n",
    "        chunk_attention_mask = attention_mask[i * chunk_size: (i + 1) * chunk_size]\n",
    "\n",
    "        # Padding to ensure all chunks have the same size (chunk_size)\n",
    "        if len(chunk_input_ids) < chunk_size:\n",
    "            padding_length = chunk_size - len(chunk_input_ids)\n",
    "            chunk_input_ids = torch.cat([chunk_input_ids, torch.zeros(padding_length, dtype=torch.long).to(device)], dim=0)\n",
    "            chunk_attention_mask = torch.cat([chunk_attention_mask, torch.zeros(padding_length, dtype=torch.long).to(device)], dim=0)\n",
    "        \n",
    "        chunk_inputs[\"input_ids\"][i] = chunk_input_ids\n",
    "        chunk_inputs[\"attention_mask\"][i] = chunk_attention_mask\n",
    "\n",
    "    # Debug print to check chunks\n",
    "    print(chunk_inputs)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**chunk_inputs)\n",
    "    \n",
    "    # Average logits before softmax for better aggregation\n",
    "    avg_logits = outputs.logits.mean(dim=0)\n",
    "    probabilities = torch.nn.functional.softmax(avg_logits, dim=-1)\n",
    "    sentiment_index = probabilities.argmax().item()\n",
    "    sentiment_class = sentiment_map[sentiment_index]\n",
    "\n",
    "    # Get top-k contributing chunks (using logits or probabilities)\n",
    "    chunk_probs = torch.nn.functional.softmax(outputs.logits, dim=-1)[:, sentiment_index]\n",
    "    top_k = 2\n",
    "    contribute_indices = chunk_probs.topk(top_k).indices.tolist()\n",
    "\n",
    "    # Decode contributing chunks\n",
    "    contribute_texts = []\n",
    "    for idx in contribute_indices:\n",
    "        chunk_tokens = chunk_inputs[\"input_ids\"][idx]  # Get the actual chunk from the padded tensor\n",
    "        chunk_text = tokenizer.decode(chunk_tokens.tolist(), skip_special_tokens=True)  # Convert to list before decoding\n",
    "        contribute_texts.append(chunk_text)\n",
    "\n",
    "    return sentiment_class, probabilities, contribute_indices, contribute_texts\n",
    "\n",
    "    # return chunk_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers[0].batch_encode_plus([\"hi how are you\", \"i am fine\", \"thanks\", \"bye\"], truncation=False, padding=True, max_length=512, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizers[0].encode_plus(\"hi hi hi hi hi hi\", truncation=False, return_tensors=\"pt\").to(device)\n",
    "chunk_input_ids = inputs['input_ids'].squeeze(0)\n",
    "chunk_attention_mask = inputs['attention_mask'].squeeze(0)\n",
    "torch.ones_like(chunk_input_ids).to(device), chunk_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "chunk_size = 3\n",
    "chunk_input_ids[i * chunk_size: (i + 1) * chunk_size], chunk_attention_mask[i * chunk_size: (i + 1) * chunk_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the DataFrame from a CSV file\n",
    "df = pd.read_csv(\"data/data-1735829992.csv\")#, encoding=\"utf-8\")\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.sort_values(by=['text'], key=lambda col: col.str.len()).iloc[0].text\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizers[0](df.text.iloc[:2].to_list(), return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_batch_with_chunking(models, tokenizers, df.text.iloc[:512].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_batch(models, tokenizers, df.text.iloc[4:14].to_list(), batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_batch(models, tokenizers, df.text.iloc[:500].to_list(), 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentiment(models, tokenizers, [\"Hi, I am a very happy person\", \"I am a very sad person\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
