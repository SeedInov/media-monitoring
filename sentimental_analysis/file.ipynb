{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"models/huggingface\"\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"models/huggingface/hub\"\n",
    "os.environ[\"HF_ASSETS_CACHE\"] = \"models/huggingface/assets\"\n",
    "os.environ[\"HF_TOKEN_PATH\"] = \"models/huggingface/token\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "model_name = \"tabularisai/multilingual-sentiment-analysis\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "classification_map = {\n",
    "    0: \"Very Negative\", \n",
    "    1: \"Negative\", \n",
    "    2: \"Neutral\", \n",
    "    3: \"Positive\", \n",
    "    4: \"Very Positive\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w, x, y, z = predict_sentiment_single_record(models, tokenizers, \"happy yipee HURRAH happy yayyyy thank you yayyyyy happy happy happy happy yayyyy thank you yayyyyy BAD BAD\")\n",
    "# w, x, y , z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_single_record(\n",
    "        model: PreTrainedModel, \n",
    "        tokenizer: PreTrainedTokenizerFast,\n",
    "        classification_map: dict[int, str],\n",
    "        text: str, \n",
    "        chunk_size: int = 512,\n",
    "        top_k: int = 1,\n",
    "        device: str = \"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Predicts sentiment for a single text record using a pre-trained model and tokenizer.\n",
    "    The text is split into chunks for processing, and the top-k contributing chunks are identified.\n",
    "\n",
    "    Args:\n",
    "        model (PreTrainedModel): Pre-trained sentiment analysis model.\n",
    "        tokenizer (PreTrainedTokenizerFast): Tokenizer for the model.\n",
    "        classification_map (dict[int, str]): Dictionary of all classes\n",
    "        text (str): Input text to analyze.\n",
    "        chunk_size (int): Size of each chunk for processing. Default is 512.\n",
    "        top_k (int): Number of top contributing chunks to return. Default is 1.\n",
    "        device (str): Device to run the model on (e.g., \"cuda\" or \"cpu\"). Default is \"cpu\" if available.\n",
    "\n",
    "    Returns:\n",
    "        predicted_sentiment (str): Predicted sentiment label.\n",
    "        probabilities (torch.Tensor): Probabilities for each sentiment class.\n",
    "        contribute_chunk_text_positions (List[Tuple[int, int]]): Start and end positions of the top-k chunks in the original text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize and move inputs to target device\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        truncation=False,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=False,\n",
    "        return_offsets_mapping=True  # Enable offset tracking\n",
    "    )\n",
    "    input_ids = inputs[\"input_ids\"].squeeze(0).to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].squeeze(0).to(device)\n",
    "    offset_mapping = inputs[\"offset_mapping\"].squeeze(0)\n",
    "    \n",
    "    # Adjust chunk_size if the text is shorter\n",
    "    original_length = len(input_ids)\n",
    "    if original_length < chunk_size:\n",
    "        chunk_size = original_length\n",
    "\n",
    "    # Calculate padding needed for full chunks\n",
    "    pad_length = (chunk_size - (original_length % chunk_size)) % chunk_size\n",
    "    total_chunks = (original_length + pad_length) // chunk_size\n",
    "    \n",
    "    # Adjust top_k if it exceeds the number of chunks\n",
    "    top_k = min(top_k, total_chunks)\n",
    "\n",
    "    # Vectorized padding and chunking\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    input_ids_padded = torch.cat([\n",
    "        input_ids,\n",
    "        torch.full((pad_length,), pad_token_id, dtype=torch.long, device=device)\n",
    "    ])\n",
    "    attention_mask_padded = torch.cat([\n",
    "        attention_mask,\n",
    "        torch.zeros(pad_length, dtype=torch.long, device=device)\n",
    "    ])\n",
    "\n",
    "    # Create chunks through reshaping\n",
    "    chunk_inputs = {\n",
    "        'input_ids': input_ids_padded.view(total_chunks, chunk_size),\n",
    "        'attention_mask': attention_mask_padded.view(total_chunks, chunk_size)\n",
    "    }\n",
    "\n",
    "    # Debug print to check chunks\n",
    "    # print(chunk_inputs)\n",
    "\n",
    "    # Model inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**chunk_inputs)\n",
    "    \n",
    "    # Average logits across chunks and compute probabilities and predictions\n",
    "    avg_logits = outputs.logits.mean(dim=0)\n",
    "    probabilities = torch.nn.functional.softmax(avg_logits, dim=-1)\n",
    "    predicted_label_idx = probabilities.argmax().item()\n",
    "    predicted_class = classification_map[predicted_label_idx]\n",
    "\n",
    "    # Get top-k chunks using raw logits (avoids second softmax)\n",
    "    chunk_contributions = outputs.logits[:, predicted_label_idx]\n",
    "    topk_indices = chunk_contributions.topk(top_k).indices.cpu().tolist()\n",
    "\n",
    "    # Map chunk indices to text positions\n",
    "    contribute_positions: list[tuple[int, int]] = []\n",
    "    for idx in topk_indices:\n",
    "        chunk_start = idx * chunk_size\n",
    "        chunk_end = chunk_start + chunk_size\n",
    "        chunk_offsets = offset_mapping[chunk_start:chunk_end]\n",
    "        \n",
    "        # Find non-padding tokens\n",
    "        non_pad = (chunk_offsets != 0).any(dim=1)\n",
    "        if non_pad.any():\n",
    "            start_pos = chunk_offsets[non_pad][0][0].item() # First non-pad token\n",
    "            end_pos = chunk_offsets[non_pad][-1][1].item()  # Last non-pad token\n",
    "        else:\n",
    "            start_pos, end_pos = 0, 0\n",
    "        contribute_positions.append((start_pos, end_pos))\n",
    "    \n",
    "    return predicted_class, contribute_positions, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    def clear_vram():\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    column = \"text\"\n",
    "    df = pd.read_csv(\"data/data-1735829992.csv\", encoding=\"utf-8\").head(10)\n",
    "    df[\"sentiment\"] = None\n",
    "    df[\"sentiment_indicator_indexes\"] = None\n",
    "    texts = df[column]\n",
    "    for i in range(len(texts)):\n",
    "        sentiment, sentiment_indicator_indexes, probabilities = classify_single_record(\n",
    "            model=model, \n",
    "            tokenizer=tokenizer, \n",
    "            classification_map=classification_map, \n",
    "            text=texts[i], \n",
    "            chunk_size=512, \n",
    "            top_k=2, \n",
    "            device=device\n",
    "        )\n",
    "        df.at[i, \"sentiment\"] = sentiment\n",
    "        df.at[i, \"sentiment_indicator_indexes\"] = sentiment_indicator_indexes\n",
    "        \n",
    "\n",
    "        clear_vram()\n",
    "    \n",
    "    clear_vram()\n",
    "\n",
    "    display(df[[\"title\", \"sentiment\", \"sentiment_indicator_indexes\"]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
