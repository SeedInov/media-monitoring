{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"models/huggingface\"\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"models/huggingface/hub\"\n",
    "os.environ[\"HF_ASSETS_CACHE\"] = \"models/huggingface/assets\"\n",
    "os.environ[\"HF_TOKEN_PATH\"] = \"models/huggingface/token\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "model_name = \"tabularisai/multilingual-sentiment-analysis\"\n",
    "\n",
    "tokenizers = AutoTokenizer.from_pretrained(model_name)\n",
    "models = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "sentiment_map = {0: \"Very Negative\", 1: \"Negative\", 2: \"Neutral\", 3: \"Positive\", 4: \"Very Positive\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, x, y, z = predict_sentiment_single_record(models, tokenizers, \"happy yipee HURRAH happy yayyyy thank you yayyyyy happy happy happy happy yayyyy thank you yayyyyy BAD BAD\")\n",
    "w, x, y , z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment_single_record(\n",
    "        model: PreTrainedModel, \n",
    "        tokenizer: PreTrainedTokenizerFast,\n",
    "        text: str, \n",
    "        chunk_size: int = 10,\n",
    "        top_k: int = 2,\n",
    "        device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "):\n",
    "    \"\"\"\n",
    "    Predicts sentiment for a single text record using a pre-trained model and tokenizer.\n",
    "    The text is split into chunks for processing, and the top-k contributing chunks are identified.\n",
    "\n",
    "    Args:\n",
    "        model (PreTrainedModel): Pre-trained sentiment analysis model.\n",
    "        tokenizer (PreTrainedTokenizerFast): Tokenizer for the model.\n",
    "        text (str): Input text to analyze.\n",
    "        chunk_size (int): Size of each chunk for processing. Default is 10.\n",
    "        top_k (int): Number of top contributing chunks to return. Default is 2.\n",
    "        device (torch.device): Device to run the model on (e.g., \"cuda\" or \"cpu\"). Default is \"cuda\" if available.\n",
    "\n",
    "    Returns:\n",
    "        predicted_sentiment (str): Predicted sentiment label.\n",
    "        probabilities (torch.Tensor): Probabilities for each sentiment class.\n",
    "        contribute_chunk_indices (List[int]): Indices of the top-k contributing chunks.\n",
    "        contribute_chunk_text_positions (List[Tuple[int, int]]): Start and end positions of the top-k chunks in the original text.\n",
    "    \"\"\"\n",
    "\n",
    "    sentiment_map = {\n",
    "        0: \"Very Negative\", \n",
    "        1: \"Negative\", \n",
    "        2: \"Neutral\", \n",
    "        3: \"Positive\", \n",
    "        4: \"Very Positive\"\n",
    "    }\n",
    "\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        truncation=False,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=False,\n",
    "        return_offsets_mapping=True  # Enable offset tracking\n",
    "    )\n",
    "    input_ids = inputs[\"input_ids\"].squeeze(0)\n",
    "    attention_mask = inputs[\"attention_mask\"].squeeze(0)\n",
    "    offset_mapping = inputs[\"offset_mapping\"].squeeze(0)\n",
    "\n",
    "    # If the text is shorter than the chunk size, adjust the chunk size\n",
    "    if len(input_ids) < chunk_size:\n",
    "        chunk_size = len(input_ids)\n",
    "    \n",
    "    # Calculate the number of chunks needed\n",
    "    total_chunks = (len(input_ids) + chunk_size - 1) // chunk_size \n",
    "\n",
    "    # Prepare tensors for chunked input\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    chunk_inputs = {\n",
    "        'input_ids': torch.zeros(total_chunks, chunk_size, dtype=torch.long, device=device), \n",
    "        'attention_mask': torch.zeros(total_chunks, chunk_size, dtype=torch.long, device=device)\n",
    "    }\n",
    "\n",
    "    # Split input into chunks and pad if necessary\n",
    "    for i in range(total_chunks):\n",
    "        start = i * chunk_size\n",
    "        end = (i + 1) * chunk_size\n",
    "        chunk_input_ids = input_ids[start: end]\n",
    "        chunk_attention_mask = attention_mask[start: end]\n",
    "\n",
    "        # Pad chunks to ensure uniform size\n",
    "        if len(chunk_input_ids) < chunk_size:\n",
    "            padding_length = chunk_size - len(chunk_input_ids)\n",
    "\n",
    "            chunk_input_ids = torch.cat([\n",
    "                chunk_input_ids, \n",
    "                torch.full((padding_length,), pad_token_id, dtype=torch.long)\n",
    "            ], dim=0)\n",
    "            chunk_attention_mask = torch.cat([\n",
    "                chunk_attention_mask, \n",
    "                torch.zeros(padding_length, dtype=torch.long)\n",
    "            ], dim=0)\n",
    "        \n",
    "        chunk_inputs[\"input_ids\"][i] = chunk_input_ids\n",
    "        chunk_inputs[\"attention_mask\"][i] = chunk_attention_mask\n",
    "    \n",
    "\n",
    "    # Debug print to check chunks\n",
    "    print(chunk_inputs)\n",
    "\n",
    "    # Run the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**chunk_inputs)\n",
    "    \n",
    "    # Average logits across chunks and compute probabilities\n",
    "    avg_logits = outputs.logits.mean(dim=0)\n",
    "    probabilities = torch.nn.functional.softmax(avg_logits, dim=-1)\n",
    "    predicted_label_idx = probabilities.argmax().item()\n",
    "    predicted_sentiment = sentiment_map[predicted_label_idx]\n",
    "\n",
    "    # Get top-k contributing chunks using logits\n",
    "    chunk_probs = torch.nn.functional.softmax(outputs.logits, dim=-1)[:, predicted_label_idx]\n",
    "    contribute_chunk_indices = chunk_probs.topk(top_k).indices.tolist()\n",
    "\n",
    "    # Extract positions of contributing chunks in the original text\n",
    "    contribute_chunk_text_positions: list[tuple[int, int]] = []\n",
    "    for idx in contribute_chunk_indices:\n",
    "        chunk_offsets = offset_mapping[idx * chunk_size: (idx + 1) * chunk_size]\n",
    "        non_pad = (chunk_offsets != 0).any(dim=1)\n",
    "        if non_pad.any():\n",
    "            start, _ = chunk_offsets[non_pad][0]  # First non-pad token\n",
    "            _, end = chunk_offsets[non_pad][-1]   # Last non-pad token\n",
    "            contribute_chunk_text_positions.append((start.item(), end.item()))\n",
    "        else:\n",
    "            # If the chunk is all padding, skip it\n",
    "            contribute_chunk_text_positions.append((0, 0))\n",
    "    \n",
    "    return predicted_sentiment, probabilities, contribute_chunk_indices, contribute_chunk_text_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers[0].batch_encode_plus([\"hi how are you\", \"i am fine\", \"thanks\", \"bye\"], truncation=False, padding=True, max_length=512, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizers[0].encode_plus(\"hi hi hi hi hi hi\", truncation=False, return_tensors=\"pt\").to(device)\n",
    "chunk_input_ids = inputs['input_ids'].squeeze(0)\n",
    "chunk_attention_mask = inputs['attention_mask'].squeeze(0)\n",
    "torch.ones_like(chunk_input_ids).to(device), chunk_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "chunk_size = 3\n",
    "chunk_input_ids[i * chunk_size: (i + 1) * chunk_size], chunk_attention_mask[i * chunk_size: (i + 1) * chunk_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the DataFrame from a CSV file\n",
    "df = pd.read_csv(\"data/data-1735829992.csv\")#, encoding=\"utf-8\")\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.sort_values(by=['text'], key=lambda col: col.str.len()).iloc[0].text\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizers[0](df.text.iloc[:2].to_list(), return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_batch_with_chunking(models, tokenizers, df.text.iloc[:512].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_batch(models, tokenizers, df.text.iloc[4:14].to_list(), batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_batch(models, tokenizers, df.text.iloc[:500].to_list(), 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentiment(models, tokenizers, [\"Hi, I am a very happy person\", \"I am a very sad person\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
