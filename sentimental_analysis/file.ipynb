{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"models/huggingface\"\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"models/huggingface/hub\"\n",
    "os.environ[\"HF_ASSETS_CACHE\"] = \"models/huggingface/assets\"\n",
    "os.environ[\"HF_TOKEN_PATH\"] = \"models/huggingface/token\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "model_name = \"tabularisai/multilingual-sentiment-analysis\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "classification_map = {\n",
    "    0: \"Very Negative\", \n",
    "    1: \"Negative\", \n",
    "    2: \"Neutral\", \n",
    "    3: \"Positive\", \n",
    "    4: \"Very Positive\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w, x, y, z = predict_sentiment_single_record(models, tokenizers, \"happy yipee HURRAH happy yayyyy thank you yayyyyy happy happy happy happy yayyyy thank you yayyyyy BAD BAD\")\n",
    "# w, x, y , z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_single_record(\n",
    "        model: PreTrainedModel, \n",
    "        tokenizer: PreTrainedTokenizerFast,\n",
    "        classification_map: dict[int, str],\n",
    "        text: str, \n",
    "        chunk_size: int = 512,\n",
    "        top_k: int = 1,\n",
    "        device: str = \"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Predicts sentiment for a single text record using a pre-trained model and tokenizer.\n",
    "    The text is split into chunks for processing, and the top-k contributing chunks are identified.\n",
    "\n",
    "    Args:\n",
    "        model (PreTrainedModel): Pre-trained sentiment analysis model.\n",
    "        tokenizer (PreTrainedTokenizerFast): Tokenizer for the model.\n",
    "        classification_map (dict[int, str]): Dictionary of all classes\n",
    "        text (str): Input text to analyze.\n",
    "        chunk_size (int): Size of each chunk for processing. Default is 512.\n",
    "        top_k (int): Number of top contributing chunks to return. Default is 1.\n",
    "        device (str): Device to run the model on (e.g., \"cuda\" or \"cpu\"). Default is \"cpu\" if available.\n",
    "\n",
    "    Returns:\n",
    "        predicted_sentiment (str): Predicted sentiment label.\n",
    "        probabilities (torch.Tensor): Probabilities for each sentiment class.\n",
    "        contribute_chunk_text_positions (List[Tuple[int, int]]): Start and end positions of the top-k chunks in the original text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        truncation=False,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=False,\n",
    "        return_offsets_mapping=True  # Enable offset tracking\n",
    "    )\n",
    "    input_ids = inputs[\"input_ids\"].squeeze(0)\n",
    "    attention_mask = inputs[\"attention_mask\"].squeeze(0)\n",
    "    offset_mapping = inputs[\"offset_mapping\"].squeeze(0)\n",
    "\n",
    "    # If the text is shorter than the chunk size, adjust the chunk size\n",
    "    if len(input_ids) < chunk_size:\n",
    "        chunk_size = len(input_ids)\n",
    "    \n",
    "    # Calculate the number of chunks needed\n",
    "    total_chunks = (len(input_ids) + chunk_size - 1) // chunk_size\n",
    "\n",
    "    # Adjust top_k if it exceeds the number of chunks\n",
    "    top_k = min(top_k, total_chunks)\n",
    "\n",
    "    # Prepare tensors for chunked input\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    chunk_inputs = {\n",
    "        'input_ids': torch.zeros(total_chunks, chunk_size, dtype=torch.long, device=device), \n",
    "        'attention_mask': torch.zeros(total_chunks, chunk_size, dtype=torch.long, device=device)\n",
    "    }\n",
    "\n",
    "    # Split input into chunks and pad if necessary\n",
    "    for i in range(total_chunks):\n",
    "        start = i * chunk_size\n",
    "        end = (i + 1) * chunk_size\n",
    "        chunk_input_ids = input_ids[start: end]\n",
    "        chunk_attention_mask = attention_mask[start: end]\n",
    "\n",
    "        # Pad chunks to ensure uniform size\n",
    "        if len(chunk_input_ids) < chunk_size:\n",
    "            padding_length = chunk_size - len(chunk_input_ids)\n",
    "\n",
    "            chunk_input_ids = torch.cat([\n",
    "                chunk_input_ids, \n",
    "                torch.full((padding_length,), pad_token_id, dtype=torch.long)\n",
    "            ], dim=0)\n",
    "            chunk_attention_mask = torch.cat([\n",
    "                chunk_attention_mask, \n",
    "                torch.zeros(padding_length, dtype=torch.long)\n",
    "            ], dim=0)\n",
    "        \n",
    "        chunk_inputs[\"input_ids\"][i] = chunk_input_ids\n",
    "        chunk_inputs[\"attention_mask\"][i] = chunk_attention_mask\n",
    "    \n",
    "\n",
    "    # Debug print to check chunks\n",
    "    # print(chunk_inputs)\n",
    "\n",
    "    # Run the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**chunk_inputs)\n",
    "    \n",
    "    # Average logits across chunks and compute probabilities\n",
    "    avg_logits = outputs.logits.mean(dim=0)\n",
    "    probabilities = torch.nn.functional.softmax(avg_logits, dim=-1)\n",
    "    predicted_label_idx = probabilities.argmax().item()\n",
    "    predicted_class = classification_map[predicted_label_idx]\n",
    "\n",
    "    # Get top-k contributing chunks using logits\n",
    "    chunk_probs = torch.nn.functional.softmax(outputs.logits, dim=-1)[:, predicted_label_idx]\n",
    "    contribute_chunk_indices = chunk_probs.topk(top_k).indices.tolist()\n",
    "\n",
    "    # Extract positions of contributing chunks in the original text\n",
    "    contribute_chunk_text_positions: list[tuple[int, int]] = []\n",
    "    for idx in contribute_chunk_indices:\n",
    "        chunk_offsets = offset_mapping[idx * chunk_size: (idx + 1) * chunk_size]\n",
    "        non_pad = (chunk_offsets != 0).any(dim=1)\n",
    "        if non_pad.any():\n",
    "            start, _ = chunk_offsets[non_pad][0]  # First non-pad token\n",
    "            _, end = chunk_offsets[non_pad][-1]   # Last non-pad token\n",
    "            contribute_chunk_text_positions.append((start.item(), end.item()))\n",
    "        else:\n",
    "            # If the chunk is all padding, skip it\n",
    "            contribute_chunk_text_positions.append((0, 0))\n",
    "    \n",
    "    return predicted_class, contribute_chunk_text_positions, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/data-1735829992.csv\", encoding=\"utf-8\")\n",
    "print(df.iloc[-2][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    def clear_vram():\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    column = \"text\"\n",
    "    df = pd.read_csv(\"data/data-1735829992.csv\", encoding=\"utf-8\")\n",
    "    df[\"sentiment\"] = None\n",
    "    df[\"sentiment_indicator_indexes\"] = None\n",
    "    texts = df[column]\n",
    "    for i in range(len(texts)):\n",
    "        sentiment, sentiment_indicator_indexes, probabilities = classify_single_record(\n",
    "            model=model, \n",
    "            tokenizer=tokenizer, \n",
    "            classification_map=classification_map, \n",
    "            text=texts[i], \n",
    "            chunk_size=512, \n",
    "            top_k=2, \n",
    "            device=device\n",
    "        )\n",
    "        df.at[i, \"sentiment\"] = sentiment\n",
    "        df.at[i, \"sentiment_indicator_indexes\"] = sentiment_indicator_indexes\n",
    "        \n",
    "\n",
    "        clear_vram()\n",
    "    \n",
    "    clear_vram()\n",
    "\n",
    "    display(df[[\"title\", \"sentiment\", \"sentiment_indicator_indexes\"]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
